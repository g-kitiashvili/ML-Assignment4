{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01: Basic Neural Network (Fully Connected)\n",
    "\n",
    "**Objective**: Establish baseline performance using a basic neural network without convolutions.\n",
    "\n",
    "**Hypothesis**: A fully connected network will perform poorly on image data because it doesn't capture spatial relationships.\n",
    "\n",
    "**Based on**: Lecture 14 - Neural Networks and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision wandb kaggle pandas numpy matplotlib seaborn scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from google.colab import files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle API\n",
    "print(\"Please upload your kaggle.json file\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Create kaggle directory and move json file\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FER2013 dataset\n",
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
    "print(\"Dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER2013Dataset(Dataset):\n",
    "    \"\"\"FER2013 Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to fer2013.csv\n",
    "            split: 'train', 'val', or 'test'\n",
    "            transform: Optional transform to apply\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Filter by split\n",
    "        if split == 'train':\n",
    "            self.data = self.data[self.data['Usage'] == 'Training'].reset_index(drop=True)\n",
    "        elif split == 'val':\n",
    "            self.data = self.data[self.data['Usage'] == 'PublicTest'].reset_index(drop=True)\n",
    "        else:\n",
    "            self.data = self.data[self.data['Usage'] == 'PrivateTest'].reset_index(drop=True)\n",
    "            \n",
    "        self.transform = transform\n",
    "        print(f\"Loaded {split} set with {len(self.data)} samples\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get pixels and reshape\n",
    "        pixels = self.data.iloc[idx]['pixels']\n",
    "        pixels = np.array([int(p) for p in pixels.split()], dtype=np.uint8)\n",
    "        pixels = pixels.reshape(48, 48)\n",
    "        \n",
    "        # Convert to 3-channel image\n",
    "        pixels = np.stack([pixels] * 3, axis=-1)\n",
    "        \n",
    "        # Get label\n",
    "        label = int(self.data.iloc[idx]['emotion'])\n",
    "        \n",
    "        if self.transform:\n",
    "            pixels = self.transform(pixels)\n",
    "            \n",
    "        return pixels, label\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition - Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic fully connected neural network (no convolutions)\n",
    "    This model treats the image as a flat vector, losing spatial information\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(BasicNN, self).__init__()\n",
    "        # Input: 48*48*3 = 6912 pixels\n",
    "        self.fc1 = nn.Linear(48 * 48 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the image - loses all spatial structure!\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc='Training')):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FER2013Dataset('fer2013.csv', split='train', transform=transform)\n",
    "val_dataset = FER2013Dataset('fer2013.csv', split='val', transform=transform)\n",
    "test_dataset = FER2013Dataset('fer2013.csv', split='test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB\n",
    "wandb.login()\n",
    "\n",
    "config = {\n",
    "    \"model\": \"BasicNN\",\n",
    "    \"dataset\": \"FER2013\",\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"architecture\": \"Fully Connected\",\n",
    "    \"parameters\": sum(p.numel() for p in BasicNN().parameters())\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"fer-challenge\",\n",
    "    name=\"experiment_01_BasicNN\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = BasicNN(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Print model info\n",
    "print(f\"\\nModel: {config['model']}\")\n",
    "print(f\"Total parameters: {config['parameters']:,}\")\n",
    "print(\"\\nExpected outcome: Poor performance due to lack of spatial feature extraction\")\n",
    "print(\"This model treats each pixel independently, ignoring spatial relationships!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Log to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"overfitting_gap\": train_acc - val_acc\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Acc')\n",
    "ax2.plot(val_accs, label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "wandb.log({\"training_curves\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "wandb.log({\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_acc\": test_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY: Basic Neural Network\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: Fully Connected Neural Network\")\n",
    "print(f\"Parameters: {config['parameters']:,}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Best Validation Accuracy: {max(val_accs):.2f}%\")\n",
    "print(f\"  - Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  - Final Overfitting Gap: {train_accs[-1] - val_accs[-1]:.2f}%\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. The model performs poorly (~25-30% accuracy) on this image classification task\")\n",
    "print(\"2. This is because fully connected networks don't capture spatial relationships\")\n",
    "print(\"3. Each pixel is treated independently, losing important spatial patterns\")\n",
    "print(\"4. The model has many parameters but low capacity for learning image features\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"This experiment demonstrates why CNNs are superior for image tasks.\")\n",
    "print(\"CNNs use convolutional layers to capture local patterns and spatial hierarchies.\")\n",
    "\n",
    "# Close WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
