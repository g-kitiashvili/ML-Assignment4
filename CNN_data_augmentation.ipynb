{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_g7VSgeCtz"
      },
      "source": [
        "# Experiment 05: CNN with Data Augmentation\n",
        "## Objective: Apply data augmentation techniques to improve model generalization and reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D_YUJ_WeCvM"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install wandb -q\n",
        "!pip install kaggle -q\n",
        "!pip install albumentations -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IKYDsm3eCvO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy2zIfSreCvS"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional - for saving results)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnxxB68qeCvT"
      },
      "outputs": [],
      "source": [
        "# Setup kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3od-1b4CeCvU"
      },
      "outputs": [],
      "source": [
        "# Download FER2013 dataset from Kaggle\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "# Extract the dataset\n",
        "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7BQOua1eCvX"
      },
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "wandb.login()\n",
        "run = wandb.init(\n",
        "    project=\"fer-challenge\",\n",
        "    name=\"exp05-cnn-data-augmentation\",\n",
        "    config={\n",
        "        \"architecture\": \"CNN with Data Augmentation\",\n",
        "        \"dataset\": \"FER2013\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": 64,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"weight_decay\": 0.0001,\n",
        "        \"data_augmentation\": True,\n",
        "        \"augmentation_techniques\": [\"rotation\", \"horizontal_flip\", \"brightness\", \"contrast\", \"gaussian_noise\"],\n",
        "        \"conv_channels\": [64, 128, 256, 256],\n",
        "        \"fc_sizes\": [512, 256],\n",
        "        \"num_classes\": 7\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5fte5SleCvZ"
      },
      "outputs": [],
      "source": [
        "# Load and explore the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
        "print(\"\\nEmotion distribution:\")\n",
        "print(train_df['emotion'].value_counts().sort_index())\n",
        "\n",
        "icml_df = pd.read_csv('icml_face_data.csv')\n",
        "\n",
        "# Split ICML data based on 'Usage'\n",
        "icml_train = icml_df[icml_df[' Usage'] == 'Training']\n",
        "icml_test = icml_df[icml_df[' Usage'].isin(['PublicTest', 'Other'])]\n",
        "\n",
        "# Drop the 'Usage' column (not needed after splitting)\n",
        "icml_train = icml_train.drop(columns=[' Usage'])\n",
        "icml_test = icml_test.drop(columns=[' Usage'])\n",
        "\n",
        "# Merge datasets\n",
        "train_df = pd.concat([train_df, icml_train], ignore_index=True)\n",
        "test_df = pd.concat([test_df, icml_test], ignore_index=True)\n",
        "\n",
        "# **Added data type check and filtering**\n",
        "print(\"\\nChecking 'pixels' column data types...\")\n",
        "initial_train_rows = len(train_df)\n",
        "initial_test_rows = len(test_df)\n",
        "\n",
        "train_df = train_df[train_df['pixels'].apply(lambda x: isinstance(x, str))]\n",
        "test_df = test_df[test_df['pixels'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "print(f\"Removed {initial_train_rows - len(train_df)} rows from training set due to non-string 'pixels'.\")\n",
        "print(f\"Removed {initial_test_rows - len(test_df)} rows from test set due to non-string 'pixels'.\")\n",
        "\n",
        "# Shuffle the merged datasets (optional but recommended)\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Output shapes and emotion distribution\n",
        "print(\"\\nMerged Train shape (after filtering):\", train_df.shape)\n",
        "print(\"Merged Test shape (after filtering):\", test_df.shape)\n",
        "\n",
        "print(\"\\nEmotion distribution in merged train set:\")\n",
        "print(train_df['emotion'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nEmotion distribution in merged test set:\")\n",
        "print(test_df['emotion'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TinOcncIeCva"
      },
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(8):\n",
        "    idx = np.random.randint(0, len(train_df))\n",
        "    pixels = train_df.iloc[idx]['pixels']\n",
        "    emotion = train_df.iloc[idx]['emotion']\n",
        "\n",
        "    # Convert pixel string to array and reshape\n",
        "    pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8)\n",
        "    pixels = pixels.reshape(48, 48)\n",
        "\n",
        "    axes[i].imshow(pixels, cmap='gray')\n",
        "    axes[i].set_title(f'{emotion_labels[emotion]}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from FER2013 Dataset')\n",
        "plt.tight_layout()\n",
        "wandb.log({\"sample_images\": wandb.Image(plt)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2JAUa9ReCvb"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation transforms\n",
        "class AugmentedFERDataset(Dataset):\n",
        "    def __init__(self, dataframe, is_training=True):\n",
        "        self.data = dataframe\n",
        "        self.is_training = is_training\n",
        "        \n",
        "        # Define augmentation pipeline for training\n",
        "        if is_training:\n",
        "            self.transform = A.Compose([\n",
        "                A.Rotate(limit=15, p=0.5),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
        "                A.Normalize(mean=[0.0], std=[1.0]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        else:\n",
        "            # No augmentation for validation/test\n",
        "            self.transform = A.Compose([\n",
        "                A.Normalize(mean=[0.0], std=[1.0]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx]['pixels']\n",
        "        emotion = self.data.iloc[idx]['emotion']\n",
        "\n",
        "        # Convert pixel string to numpy array\n",
        "        pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8)\n",
        "        pixels = pixels.reshape(48, 48)\n",
        "        \n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=pixels)\n",
        "            pixels = augmented['image']\n",
        "            \n",
        "        # Ensure tensor is float and has correct shape (1, 48, 48)\n",
        "        if len(pixels.shape) == 2:\n",
        "            pixels = pixels.unsqueeze(0)\n",
        "        \n",
        "        return pixels.float(), torch.tensor(emotion, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_aug"
      },
      "outputs": [],
      "source": [
        "# Visualize augmented samples\n",
        "sample_idx = 0\n",
        "pixels = train_df.iloc[sample_idx]['pixels']\n",
        "emotion = train_df.iloc[sample_idx]['emotion']\n",
        "\n",
        "# Original image\n",
        "pixels_orig = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8)\n",
        "pixels_orig = pixels_orig.reshape(48, 48)\n",
        "\n",
        "# Create augmentation pipeline\n",
        "aug_transform = A.Compose([\n",
        "    A.Rotate(limit=15, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
        "])\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Show original\n",
        "axes[0].imshow(pixels_orig, cmap='gray')\n",
        "axes[0].set_title(f'Original - {emotion_labels[emotion]}')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Show 7 augmented versions\n",
        "for i in range(1, 8):\n",
        "    augmented = aug_transform(image=pixels_orig)\n",
        "    axes[i].imshow(augmented['image'], cmap='gray')\n",
        "    axes[i].set_title(f'Augmented {i}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Data Augmentation Examples')\n",
        "plt.tight_layout()\n",
        "wandb.log({\"augmentation_examples\": wandb.Image(plt)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bflabsmOeCvc"
      },
      "outputs": [],
      "source": [
        "# Create datasets with augmentation\n",
        "full_dataset_train = AugmentedFERDataset(train_df, is_training=True)\n",
        "\n",
        "# Split into train and validation\n",
        "train_size = int(0.8 * len(full_dataset_train))\n",
        "val_size = len(full_dataset_train) - train_size\n",
        "\n",
        "# Create indices for splitting\n",
        "indices = list(range(len(full_dataset_train)))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
        "\n",
        "# Create train subset with augmentation\n",
        "train_subset_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
        "val_subset_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
        "\n",
        "train_dataset = AugmentedFERDataset(train_subset_df, is_training=True)\n",
        "val_dataset = AugmentedFERDataset(val_subset_df, is_training=False)  # No augmentation for validation\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqz9tzV0eCvc"
      },
      "outputs": [],
      "source": [
        "# CNN Model (same architecture as previous experiments for fair comparison)\n",
        "class AugmentedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_conv=0.2, dropout_fc=0.5):\n",
        "        super(AugmentedCNN, self).__init__()\n",
        "\n",
        "        # Conv Block 1\n",
        "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.bn1_1 = nn.BatchNorm2d(64)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn1_2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_conv)\n",
        "\n",
        "        # Conv Block 2\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(128)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_conv)\n",
        "\n",
        "        # Conv Block 3\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(256)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn3_2 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_conv)\n",
        "\n",
        "        # Conv Block 4\n",
        "        self.conv4_1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn4_1 = nn.BatchNorm2d(256)\n",
        "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn4_2 = nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_conv)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_fc)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc2 = nn.Dropout(dropout_fc)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Activation\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Calculate total parameters\n",
        "        self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv Block 1\n",
        "        x = self.relu(self.bn1_1(self.conv1_1(x)))\n",
        "        x = self.relu(self.bn1_2(self.conv1_2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Conv Block 2\n",
        "        x = self.relu(self.bn2_1(self.conv2_1(x)))\n",
        "        x = self.relu(self.bn2_2(self.conv2_2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Conv Block 3\n",
        "        x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
        "        x = self.relu(self.bn3_2(self.conv3_2(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Conv Block 4\n",
        "        x = self.relu(self.bn4_1(self.conv4_1(x)))\n",
        "        x = self.relu(self.bn4_2(self.conv4_2(x)))\n",
        "        x = self.pool4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # FC layers\n",
        "        x = self.relu(self.bn_fc1(self.fc1(x)))\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.relu(self.bn_fc2(self.fc2(x)))\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-viGxgTeeCve"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = AugmentedCNN().to(device)\n",
        "print(f\"Total parameters: {model.total_params:,}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "# Log model architecture to W&B\n",
        "wandb.watch(model, log='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZ3bHDWeCve"
      },
      "outputs": [],
      "source": [
        "# Print model architecture\n",
        "print(\"Model Architecture:\")\n",
        "print(\"=\" * 50)\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:20} {param.shape}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jji8FLFIeCve"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc='Training')\n",
        "    for inputs, labels in progress_bar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': 100 * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYKU51eeCvf"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(loader, desc='Validation')\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': loss.item(),\n",
        "                'acc': 100 * correct / total\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN9Ok8bVeCvg"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    print(f'\\nEpoch {epoch+1}/30')\n",
        "    print('-' * 50)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc, predictions, labels = validate_epoch(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Log to W&B\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "        'overfitting_gap': train_acc - val_acc\n",
        "    })\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'Overfitting Gap: {train_acc - val_acc:.2f}%')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_augmented_cnn_model.pth')\n",
        "        print(f'New best model saved with validation accuracy: {val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfQfsN2aeCvg"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
        "ax1.plot(val_losses, label='Val Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "ax2.plot(train_accs, label='Train Acc', linewidth=2)\n",
        "ax2.plot(val_accs, label='Val Acc', linewidth=2)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "wandb.log({\"training_history\": wandb.Image(plt)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hncz0I7aeCvg"
      },
      "outputs": [],
      "source": [
        "# Plot overfitting gap over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "overfitting_gaps = [train_accs[i] - val_accs[i] for i in range(len(train_accs))]\n",
        "plt.plot(overfitting_gaps, 'r-', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Overfitting Gap (%)')\n",
        "plt.title('Overfitting Gap Throughout Training (Data Augmentation)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "wandb.log({\"overfitting_gap_plot\": wandb.Image(plt)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZwcaaoHeCvh"
      },
      "outputs": [],
      "source": [
        "# Load best model for final evaluation\n",
        "model.load_state_dict(torch.load('best_augmented_cnn_model.pth'))\n",
        "_, _, final_predictions, final_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(final_labels, final_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_labels,\n",
        "            yticklabels=emotion_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CNN with Data Augmentation')\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbmHpH5YeCvh"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(\"=\" * 70)\n",
        "report = classification_report(final_labels, final_predictions,\n",
        "                             target_names=emotion_labels,\n",
        "                             output_dict=True)\n",
        "print(classification_report(final_labels, final_predictions, target_names=emotion_labels))\n",
        "\n",
        "# Log per-class metrics to W&B\n",
        "for emotion in emotion_labels:\n",
        "    wandb.log({\n",
        "        f\"{emotion}_precision\": report[emotion]['precision'],\n",
        "        f\"{emotion}_recall\": report[emotion]['recall'],\n",
        "        f\"{emotion}_f1\": report[emotion]['f1-score']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMlv8C-XeCvh"
      },
      "outputs": [],
      "source": [
        "# Analyze misclassifications\n",
        "misclassified_indices = np.where(np.array(final_predictions) != np.array(final_labels))[0]\n",
        "correct_indices = np.where(np.array(final_predictions) == np.array(final_labels))[0]\n",
        "\n",
        "print(f\"\\nTotal misclassifications: {len(misclassified_indices)} out of {len(final_labels)}\")\n",
        "print(f\"Misclassification rate: {len(misclassified_indices)/len(final_labels)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QEvKNzieCvh"
      },
      "outputs": [],
      "source": [
        "# Save final model and log to W&B\n",
        "torch.save(model.state_dict(), 'final_augmented_cnn_model.pth')\n",
        "wandb.save('final_augmented_cnn_model.pth')\n",
        "wandb.save('best_augmented_cnn_model.pth')\n",
        "\n",
        "# Summary statistics\n",
        "summary_stats = {\n",
        "    \"final_train_accuracy\": train_accs[-1],\n",
        "    \"final_val_accuracy\": val_accs[-1],\n",
        "    \"best_val_accuracy\": best_val_acc,\n",
        "    \"overfitting_gap\": train_accs[-1] - val_accs[-1],\n",
        "    \"max_overfitting_gap\": max(overfitting_gaps),\n",
        "    \"total_parameters\": model.total_params,\n",
        "    \"macro_f1_score\": report['macro avg']['f1-score'],\n",
        "    \"weighted_f1_score\": report['weighted avg']['f1-score']\n",
        "}\n",
        "\n",
        "wandb.log(summary_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR3_7g-QeCvi"
      },
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT SUMMARY: CNN WITH DATA AUGMENTATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - 4 Convolutional Blocks (VGG-style)\")\n",
        "print(f\"  - Conv channels: [64, 128, 256, 256]\")\n",
        "print(f\"  - FC layers: [512, 256, 7]\")\n",
        "print(f\"  - Total Parameters: {model.total_params:,}\")\n",
        "print(f\"  - Data Augmentation: Rotation, Flip, Brightness/Contrast, Noise\")\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  - Final Training Accuracy: {train_accs[-1]:.2f}%\")\n",
        "print(f\"  - Final Validation Accuracy: {val_accs[-1]:.2f}%\")\n",
        "print(f\"  - Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"  - Final Overfitting Gap: {train_accs[-1] - val_accs[-1]:.2f}%\")\n",
        "print(f\"  - Maximum Overfitting Gap: {max(overfitting_gaps):.2f}%\")\n",
        "print(f\"  - Macro F1-Score: {report['macro avg']['f1-score']:.3f}\")\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
