{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03: CNN Overfitting Analysis\n",
    "## Objective: Build a large CNN without regularization to observe overfitting behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install wandb -q\n",
    "!pip install kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup kaggle directory\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FER2013 dataset from Kaggle\n",
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "# Extract the dataset\n",
    "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"fer-challenge\",\n",
    "    name=\"exp03-cnn-overfitting\",\n",
    "    config={\n",
    "        \"architecture\": \"Large CNN No Regularization\",\n",
    "        \"dataset\": \"FER2013\",\n",
    "        \"epochs\": 30,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"conv_channels\": [64, 128, 256, 512, 512],\n",
    "        \"fc_sizes\": [1024, 512],\n",
    "        \"num_classes\": 7,\n",
    "        \"regularization\": \"None\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
    "print(\"\\nEmotion distribution:\")\n",
    "print(train_df['emotion'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    idx = np.random.randint(0, len(train_df))\n",
    "    pixels = train_df.iloc[idx]['pixels']\n",
    "    emotion = train_df.iloc[idx]['emotion']\n",
    "    \n",
    "    # Convert pixel string to array and reshape\n",
    "    pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8)\n",
    "    pixels = pixels.reshape(48, 48)\n",
    "    \n",
    "    axes[i].imshow(pixels, cmap='gray')\n",
    "    axes[i].set_title(f'{emotion_labels[emotion]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from FER2013 Dataset')\n",
    "plt.tight_layout()\n",
    "wandb.log({\"sample_images\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pixels = self.data.iloc[idx]['pixels']\n",
    "        emotion = self.data.iloc[idx]['emotion']\n",
    "        \n",
    "        # Convert pixel string to numpy array\n",
    "        pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.float32)\n",
    "        pixels = pixels / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # For CNN, reshape to (1, 48, 48) - single channel\n",
    "        pixels = pixels.reshape(1, 48, 48)\n",
    "        \n",
    "        return torch.tensor(pixels), torch.tensor(emotion, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "full_dataset = FERDataset(train_df)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large CNN Model without regularization\n",
    "class LargeCNNNoReg(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(LargeCNNNoReg, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 48x48 -> 24x24\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 24x24 -> 12x12\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 12x12 -> 6x6\n",
    "        \n",
    "        # Conv Block 4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # 6x6 -> 3x3\n",
    "        \n",
    "        # Conv Block 5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 3 * 3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Calculate total parameters\n",
    "        self.total_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv Block 1\n",
    "        x = self.relu(self.conv1_1(x))\n",
    "        x = self.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        x = self.relu(self.conv2_1(x))\n",
    "        x = self.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        x = self.relu(self.conv3_1(x))\n",
    "        x = self.relu(self.conv3_2(x))\n",
    "        x = self.relu(self.conv3_3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Conv Block 4\n",
    "        x = self.relu(self.conv4_1(x))\n",
    "        x = self.relu(self.conv4_2(x))\n",
    "        x = self.relu(self.conv4_3(x))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Conv Block 5\n",
    "        x = self.relu(self.conv5_1(x))\n",
    "        x = self.relu(self.conv5_2(x))\n",
    "        x = self.relu(self.conv5_3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = LargeCNNNoReg().to(device)\n",
    "print(f\"Total parameters: {model.total_params:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Log model architecture to W&B\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:20} {param.shape}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc='Training')\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': 100 * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc='Validation')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': 100 * correct / total\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    print(f'\\nEpoch {epoch+1}/30')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, predictions, labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "        'overfitting_gap': train_acc - val_acc\n",
    "    })\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'Overfitting Gap: {train_acc - val_acc:.2f}%')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_large_cnn_noreg_model.pth')\n",
    "        print(f'New best model saved with validation accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "ax1.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Acc', linewidth=2)\n",
    "ax2.plot(val_accs, label='Val Acc', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "wandb.log({\"training_history\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overfitting gap over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "overfitting_gaps = [train_accs[i] - val_accs[i] for i in range(len(train_accs))]\n",
    "plt.plot(overfitting_gaps, 'r-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Overfitting Gap (%)')\n",
    "plt.title('Overfitting Gap Throughout Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "wandb.log({\"overfitting_gap_plot\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_large_cnn_noreg_model.pth'))\n",
    "_, _, final_predictions, final_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(final_labels, final_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=emotion_labels,\n",
    "            yticklabels=emotion_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Large CNN No Regularization')\n",
    "wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "report = classification_report(final_labels, final_predictions,\n",
    "                             target_names=emotion_labels,\n",
    "                             output_dict=True)\n",
    "print(classification_report(final_labels, final_predictions, target_names=emotion_labels))\n",
    "\n",
    "# Log per-class metrics to W&B\n",
    "for emotion in emotion_labels:\n",
    "    wandb.log({\n",
    "        f\"{emotion}_precision\": report[emotion]['precision'],\n",
    "        f\"{emotion}_recall\": report[emotion]['recall'],\n",
    "        f\"{emotion}_f1\": report[emotion]['f1-score']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "misclassified_indices = np.where(np.array(final_predictions) != np.array(final_labels))[0]\n",
    "correct_indices = np.where(np.array(final_predictions) == np.array(final_labels))[0]\n",
    "\n",
    "print(f\"\\nTotal misclassifications: {len(misclassified_indices)} out of {len(final_labels)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_indices)/len(final_labels)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and log to W&B\n",
    "torch.save(model.state_dict(), 'final_large_cnn_noreg_model.pth')\n",
    "wandb.save('final_large_cnn_noreg_model.pth')\n",
    "wandb.save('best_large_cnn_noreg_model.pth')\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    \"final_train_accuracy\": train_accs[-1],\n",
    "    \"final_val_accuracy\": val_accs[-1],\n",
    "    \"best_val_accuracy\": best_val_acc,\n",
    "    \"overfitting_gap\": train_accs[-1] - val_accs[-1],\n",
    "    \"max_overfitting_gap\": max(overfitting_gaps),\n",
    "    \"total_parameters\": model.total_params,\n",
    "    \"macro_f1_score\": report['macro avg']['f1-score'],\n",
    "    \"weighted_f1_score\": report['weighted avg']['f1-score']\n",
    "}\n",
    "\n",
    "wandb.log(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY: LARGE CNN WITHOUT REGULARIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  - 5 Convolutional Blocks (VGG-style)\")\n",
    "print(f\"  - Conv channels: [64, 128, 256, 512, 512]\")\n",
    "print(f\"  - FC layers: [1024, 512, 7]\")\n",
    "print(f\"  - Total Parameters: {model.total_params:,}\")\n",
    "print(f\"  - No regularization techniques applied\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  - Final Training Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"  - Final Validation Accuracy: {val_accs[-1]:.2f}%\")\n",
    "print(f\"  - Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"  - Final Overfitting Gap: {train_accs[-1] - val_accs[-1]:.2f}%\")\n",
    "print(f\"  - Maximum Overfitting Gap: {max(overfitting_gaps):.2f}%\")\n",
    "print(f\"  - Macro F1-Score: {report['macro avg']['f1-score']:.3f}\")\n",
    "\n",
    "wandb.finish()"
]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
