{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_g7VSgeCtz"
      },
      "source": [
        "# Experiment 10: Batch Size Impact\n",
        "## Objective: Compare small vs large batch sizes on facial expression recognition performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D_YUJ_WeCvM"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install wandb -q\n",
        "!pip install kaggle -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IKYDsm3eCvO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "import time\n",
        "import copy\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy2zIfSreCvS"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional - for saving results)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnxxB68qeCvT"
      },
      "outputs": [],
      "source": [
        "# Setup kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3od-1b4CeCvU"
      },
      "outputs": [],
      "source": [
        "# Download FER2013 dataset from Kaggle\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "# Extract the dataset\n",
        "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5fte5SleCvZ"
      },
      "outputs": [],
      "source": [
        "# Load and explore the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
        "print(\"\\nEmotion distribution:\")\n",
        "print(train_df['emotion'].value_counts().sort_index())\n",
        "\n",
        "icml_df = pd.read_csv('icml_face_data.csv')\n",
        "\n",
        "# Split ICML data based on 'Usage'\n",
        "icml_train = icml_df[icml_df[' Usage'] == 'Training']\n",
        "icml_test = icml_df[icml_df[' Usage'].isin(['PublicTest', 'Other'])]\n",
        "\n",
        "# Drop the 'Usage' column (not needed after splitting)\n",
        "icml_train = icml_train.drop(columns=[' Usage'])\n",
        "icml_test = icml_test.drop(columns=[' Usage'])\n",
        "\n",
        "# Merge datasets\n",
        "train_df = pd.concat([train_df, icml_train], ignore_index=True)\n",
        "test_df = pd.concat([test_df, icml_test], ignore_index=True)\n",
        "\n",
        "# **Added data type check and filtering**\n",
        "print(\"\\nChecking 'pixels' column data types...\")\n",
        "initial_train_rows = len(train_df)\n",
        "initial_test_rows = len(test_df)\n",
        "\n",
        "train_df = train_df[train_df['pixels'].apply(lambda x: isinstance(x, str))]\n",
        "test_df = test_df[test_df['pixels'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "print(f\"Removed {initial_train_rows - len(train_df)} rows from training set due to non-string 'pixels'.\")\n",
        "print(f\"Removed {initial_test_rows - len(test_df)} rows from test set due to non-string 'pixels'.\")\n",
        "\n",
        "# Shuffle the merged datasets (optional but recommended)\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Output shapes and emotion distribution\n",
        "print(\"\\nMerged Train shape (after filtering):\", train_df.shape)\n",
        "print(\"Merged Test shape (after filtering):\", test_df.shape)\n",
        "\n",
        "print(\"\\nEmotion distribution in merged train set:\")\n",
        "print(train_df['emotion'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nEmotion distribution in merged test set:\")\n",
        "print(test_df['emotion'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TinOcncIeCva"
      },
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(8):\n",
        "    idx = np.random.randint(0, len(train_df))\n",
        "    pixels = train_df.iloc[idx]['pixels']\n",
        "    emotion = train_df.iloc[idx]['emotion']\n",
        "\n",
        "    # Convert pixel string to array and reshape\n",
        "    pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8)\n",
        "    pixels = pixels.reshape(48, 48)\n",
        "\n",
        "    axes[i].imshow(pixels, cmap='gray')\n",
        "    axes[i].set_title(f'{emotion_labels[emotion]}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from FER2013 Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2JAUa9ReCvb"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class FERDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx]['pixels']\n",
        "        emotion = self.data.iloc[idx]['emotion']\n",
        "\n",
        "        # Convert pixel string to numpy array\n",
        "        pixels = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.float32)\n",
        "        pixels = pixels / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # For CNN, reshape to (1, 48, 48) - single channel\n",
        "        pixels = pixels.reshape(1, 48, 48)\n",
        "\n",
        "        return torch.tensor(pixels), torch.tensor(emotion, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bflabsmOeCvc"
      },
      "outputs": [],
      "source": [
        "# Create base dataset\n",
        "full_dataset = FERDataset(train_df)\n",
        "\n",
        "# Split into train and validation\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqz9tzV0eCvc"
      },
      "outputs": [],
      "source": [
        "# CNN Model for Batch Size Testing\n",
        "class BatchSizeTestCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(BatchSizeTestCNN, self).__init__()\n",
        "\n",
        "        # Conv Block 1\n",
        "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.bn1_1 = nn.BatchNorm2d(64)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn1_2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout2d(0.1)\n",
        "\n",
        "        # Conv Block 2\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(128)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout2d(0.2)\n",
        "\n",
        "        # Conv Block 3\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(256)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn3_2 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout2d(0.2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.dropout_fc1 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc2 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Activation\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Calculate total parameters\n",
        "        self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv Block 1\n",
        "        x = self.relu(self.bn1_1(self.conv1_1(x)))\n",
        "        x = self.relu(self.bn1_2(self.conv1_2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Conv Block 2\n",
        "        x = self.relu(self.bn2_1(self.conv2_1(x)))\n",
        "        x = self.relu(self.bn2_2(self.conv2_2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Conv Block 3\n",
        "        x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
        "        x = self.relu(self.bn3_2(self.conv3_2(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # FC layers\n",
        "        x = self.relu(self.bn_fc1(self.fc1(x)))\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.relu(self.bn_fc2(self.fc2(x)))\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-viGxgTeeCve"
      },
      "outputs": [],
      "source": [
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create a sample model to check parameter count\n",
        "sample_model = BatchSizeTestCNN().to(device)\n",
        "total_params = sample_model.total_params\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "del sample_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZ3bHDWeCve"
      },
      "outputs": [],
      "source": [
        "# Define batch size configurations\n",
        "batch_size_configs = {\n",
        "    'Small_16': {\n",
        "        'batch_size': 16,\n",
        "        'lr': 0.001,\n",
        "        'description': 'Small batch (16)',\n",
        "        'color': 'red',\n",
        "        'marker': 'o'\n",
        "    },\n",
        "    'Medium_64': {\n",
        "        'batch_size': 64,\n",
        "        'lr': 0.001,\n",
        "        'description': 'Medium batch (64)',\n",
        "        'color': 'blue',\n",
        "        'marker': 's'\n",
        "    },\n",
        "    'Large_128': {\n",
        "        'batch_size': 128,\n",
        "        'lr': 0.0015,  # Slightly higher LR for larger batch\n",
        "        'description': 'Large batch (128)',\n",
        "        'color': 'green',\n",
        "        'marker': '^'\n",
        "    },\n",
        "    'XLarge_256': {\n",
        "        'batch_size': 256,\n",
        "        'lr': 0.002,  # Higher LR for very large batch\n",
        "        'description': 'Extra Large batch (256)',\n",
        "        'color': 'purple',\n",
        "        'marker': 'D'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Batch Size Configurations:\")\n",
        "print(\"=\" * 60)\n",
        "for name, config in batch_size_configs.items():\n",
        "    print(f\"{config['description']}: Batch size={config['batch_size']}, LR={config['lr']}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jji8FLFIeCve"
      },
      "outputs": [],
      "source": [
        "# Training function with timing\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    gradient_norms = []\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Calculate gradient norm\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** (1. / 2)\n",
        "        gradient_norms.append(total_norm)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    avg_gradient_norm = np.mean(gradient_norms)\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_time, avg_gradient_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYKU51eeCvf"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_time, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN9Ok8bVeCvg"
      },
      "outputs": [],
      "source": [
        "# Train models with different batch sizes\n",
        "results = {}\n",
        "num_epochs = 20\n",
        "\n",
        "for config_name, config in batch_size_configs.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training with {config['description']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create data loaders with specific batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
        "                           shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    \n",
        "    # Initialize W&B run for this batch size\n",
        "    wandb.init(\n",
        "        project=\"fer-challenge\",\n",
        "        name=f\"exp10-batch-{config_name.lower()}\",\n",
        "        config={\n",
        "            \"architecture\": \"CNN for Batch Size Comparison\",\n",
        "            \"dataset\": \"FER2013\",\n",
        "            \"epochs\": num_epochs,\n",
        "            \"batch_size\": config['batch_size'],\n",
        "            \"learning_rate\": config['lr'],\n",
        "            \"optimizer\": \"Adam\",\n",
        "            \"weight_decay\": 1e-4,\n",
        "            \"num_classes\": 7\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Initialize model\n",
        "    model = BatchSizeTestCNN().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
        "    \n",
        "    # Track metrics\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    train_times = []\n",
        "    val_times = []\n",
        "    gradient_norms = []\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    # Log model to W&B\n",
        "    wandb.watch(model, log='all')\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {config[\"description\"]}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc, train_time, grad_norm = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        train_times.append(train_time)\n",
        "        gradient_norms.append(grad_norm)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, val_time, predictions, labels = validate_epoch(\n",
        "            model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        val_times.append(val_time)\n",
        "\n",
        "        # Log to W&B\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'train_time': train_time,\n",
        "            'val_time': val_time,\n",
        "            'gradient_norm': grad_norm,\n",
        "            'overfitting_gap': train_acc - val_acc,\n",
        "            'samples_per_second': len(train_dataset) / train_time\n",
        "        })\n",
        "\n",
        "        print(f'Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, Time={train_time:.1f}s')\n",
        "        print(f'Val: Loss={val_loss:.4f}, Acc={val_acc:.2f}%, Time={val_time:.1f}s')\n",
        "        print(f'Gradient Norm: {grad_norm:.4f}')\n",
        "        print(f'Samples/sec: {len(train_dataset)/train_time:.1f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'best_{config_name.lower()}_model.pth')\n",
        "    \n",
        "    # Store results\n",
        "    results[config_name] = {\n",
        "        'config': config,\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'train_times': train_times,\n",
        "        'val_times': val_times,\n",
        "        'gradient_norms': gradient_norms,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'final_train_acc': train_accs[-1],\n",
        "        'final_val_acc': val_accs[-1],\n",
        "        'avg_train_time': np.mean(train_times),\n",
        "        'total_train_time': sum(train_times),\n",
        "        'avg_gradient_norm': np.mean(gradient_norms),\n",
        "        'model': model\n",
        "}\n",

        "print(f\"\\n{config['description']} - Best Val Acc: {best_val_acc:.2f}%\")\n",
        "print(f\"Average training time per epoch: {np.mean(train_times):.1f}s\")\n",
        "print(f\"Total training time: {sum(train_times):.1f}s\")\n",

        "# Finish W&B run\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfQfsN2aeCvg"
      },
      "outputs": [],
      "source": [
        "# Plot comprehensive comparison of all batch sizes\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "\n",
        "# Training Loss\n",
        "for config_name, data in results.items():\n",
        "    axes[0, 0].plot(data['train_losses'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2, \n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Training Loss')\n",
        "axes[0, 0].set_title('Training Loss Comparison')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Loss\n",
        "for config_name, data in results.items():\n",
        "    axes[0, 1].plot(data['val_losses'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Validation Loss')\n",
        "axes[0, 1].set_title('Validation Loss Comparison')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Training Accuracy\n",
        "for config_name, data in results.items():\n",
        "    axes[0, 2].plot(data['train_accs'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[0, 2].set_xlabel('Epoch')\n",
        "axes[0, 2].set_ylabel('Training Accuracy (%)')\n",
        "axes[0, 2].set_title('Training Accuracy Comparison')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Accuracy\n",
        "for config_name, data in results.items():\n",
        "    axes[1, 0].plot(data['val_accs'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Validation Accuracy (%)')\n",
        "axes[1, 0].set_title('Validation Accuracy Comparison')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training Time per Epoch\n",
        "for config_name, data in results.items():\n",
        "    axes[1, 1].plot(data['train_times'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Training Time (seconds)')\n",
        "axes[1, 1].set_title('Training Time per Epoch')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient Norms\n",
        "for config_name, data in results.items():\n",
        "    axes[1, 2].plot(data['gradient_norms'], label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[1, 2].set_xlabel('Epoch')\n",
        "axes[1, 2].set_ylabel('Average Gradient Norm')\n",
        "axes[1, 2].set_title('Gradient Norm Comparison')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Overfitting Gap\n",
        "for config_name, data in results.items():\n",
        "    overfitting_gaps = [data['train_accs'][i] - data['val_accs'][i] for i in range(len(data['train_accs']))]\n",
        "    axes[2, 0].plot(overfitting_gaps, label=data['config']['description'], \n",
        "                   color=data['config']['color'], linewidth=2,\n",
        "                   marker=data['config']['marker'], markersize=4)\n",
        "axes[2, 0].set_xlabel('Epoch')\n",
        "axes[2, 0].set_ylabel('Overfitting Gap (%)')\n",
        "axes[2, 0].set_title('Overfitting Gap Comparison')\n",
        "axes[2, 0].legend()\n",
        "axes[2, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Throughput (Samples per second)\n",
        "batch_sizes = [results[config]['config']['batch_size'] for config in results.keys()]\n",
        "throughputs = [len(train_dataset) / results[config]['avg_train_time'] for config in results.keys()]\n",
        "colors = [results[config]['config']['color'] for config in results.keys()]\n",
        "labels = [results[config]['config']['description'] for config in results.keys()]\n",
        "\n",
        "bars = axes[2, 1].bar(range(len(batch_sizes)), throughputs, color=colors, alpha=0.7)\n",
        "axes[2, 1].set_xlabel('Batch Size Configuration')\n",
        "axes[2, 1].set_ylabel('Samples per Second')\n",
        "axes[2, 1].set_title('Training Throughput Comparison')\n",
        "axes[2, 1].set_xticks(range(len(batch_sizes)))\n",
        "axes[2, 1].set_xticklabels([f'{bs}' for bs in batch_sizes], rotation=45)\n",
        "axes[2, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, throughput in zip(bars, throughputs):\n",
        "    axes[2, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                    f'{throughput:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# Performance vs Batch Size Scatter\n",
        "best_accs = [results[config]['best_val_acc'] for config in results.keys()]\n",
        "markers = [results[config]['config']['marker'] for config in results.keys()]\n",
        "\n",
        "for i, (bs, acc, color, marker, label) in enumerate(zip(batch_sizes, best_accs, colors, markers, labels)):\n",
        "    axes[2, 2].scatter(bs, acc, color=color, s=100, marker=marker, label=label, alpha=0.8)\n",
        "\n",
        "axes[2, 2].set_xlabel('Batch Size')\n",
        "axes[2, 2].set_ylabel('Best Validation Accuracy (%)')\n",
        "axes[2, 2].set_title('Performance vs Batch Size')\n",
        "axes[2, 2].legend()\n",
        "axes[2, 2].grid(True, alpha=0.3)\n",
        "axes[2, 2].set_xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hncz0I7aeCvg"
      },
      "outputs": [],
      "source": [
        "# Create detailed performance comparison table\n",
        "summary_data = []\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    summary_data.append({\n",
        "        'Configuration': config['description'],\n",
        "        'Batch Size': config['batch_size'],\n",
        "        'Learning Rate': config['lr'],\n",
        "        'Best Val Acc (%)': f\"{data['best_val_acc']:.2f}\",\n",
        "        'Final Train Acc (%)': f\"{data['final_train_acc']:.2f}\",\n",
        "        'Final Val Acc (%)': f\"{data['final_val_acc']:.2f}\",\n",
        "        'Overfitting Gap (%)': f\"{data['final_train_acc'] - data['final_val_acc']:.2f}\",\n",
        "        'Avg Train Time (s)': f\"{data['avg_train_time']:.1f}\",\n",
        "        'Total Train Time (s)': f\"{data['total_train_time']:.1f}\",\n",
        "        'Throughput (samples/s)': f\"{len(train_dataset) / data['avg_train_time']:.1f}\",\n",
        "        'Avg Gradient Norm': f\"{data['avg_gradient_norm']:.4f}\",\n",
        "        'Batches per Epoch': len(train_dataset) // config['batch_size']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\nBatch Size Impact Summary:\")\n",
        "print(\"=\" * 150)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\" * 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZwcaaoHeCvh"
      },
      "outputs": [],
      "source": [
        "# Efficiency Analysis\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Create 2x2 subplot for efficiency metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Time to Convergence\n",
        "batch_sizes = [results[config]['config']['batch_size'] for config in results.keys()]\n",
        "convergence_times = []\n",
        "\n",
        "for config_name, data in results.items():\n",
        "    # Find epoch where 95% of best accuracy was reached\n",
        "    target_acc = 0.95 * data['best_val_acc']\n",
        "    convergence_epoch = next((i for i, acc in enumerate(data['val_accs']) if acc >= target_acc), num_epochs)\n",
        "    convergence_time = sum(data['train_times'][:convergence_epoch+1])\n",
        "    convergence_times.append(convergence_time)\n",
        "\n",
        "colors = [results[config]['config']['color'] for config in results.keys()]\n",
        "bars1 = axes[0, 0].bar(range(len(batch_sizes)), convergence_times, color=colors, alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Batch Size')\n",
        "axes[0, 0].set_ylabel('Time to 95% Best Accuracy (s)')\n",
        "axes[0, 0].set_title('Convergence Speed Comparison')\n",
        "axes[0, 0].set_xticks(range(len(batch_sizes)))\n",
        "axes[0, 0].set_xticklabels([f'{bs}' for bs in batch_sizes])\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, time_val in zip(bars1, convergence_times):\n",
        "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
        "                    f'{time_val:.0f}s', ha='center', va='bottom')\n",
        "\n",
        "# 2. Memory Efficiency (samples per second per batch size)\n",
        "efficiency_ratio = [throughputs[i] / batch_sizes[i] for i in range(len(batch_sizes))]\n",
        "bars2 = axes[0, 1].bar(range(len(batch_sizes)), efficiency_ratio, color=colors, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Batch Size')\n",
        "axes[0, 1].set_ylabel('Efficiency Ratio (samples/s per batch size)')\n",
        "axes[0, 1].set_title('Memory Efficiency Comparison')\n",
        "axes[0, 1].set_xticks(range(len(batch_sizes)))\n",
        "axes[0, 1].set_xticklabels([f'{bs}' for bs in batch_sizes])\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Gradient Noise (std of gradient norms)\n",
        "gradient_stds = [np.std(data['gradient_norms']) for data in results.values()]\n",
        "bars3 = axes[1, 0].bar(range(len(batch_sizes)), gradient_stds, color=colors, alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Batch Size')\n",
        "axes[1, 0].set_ylabel('Gradient Noise (std of gradient norms)')\n",
        "axes[1, 0].set_title('Gradient Stability Comparison')\n",
        "axes[1, 0].set_xticks(range(len(batch_sizes)))\n",
        "axes[1, 0].set_xticklabels([f'{bs}' for bs in batch_sizes])\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Training Stability (std of validation accuracy in last 5 epochs)\n",
        "stability_scores = [np.std(data['val_accs'][-5:]) for data in results.values()]\n",
        "bars4 = axes[1, 1].bar(range(len(batch_sizes)), stability_scores, color=colors, alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Batch Size')\n",
        "axes[1, 1].set_ylabel('Training Stability (std of last 5 val acc)')\n",
        "axes[1, 1].set_title('Training Stability Comparison')\n",
        "axes[1, 1].set_xticks(range(len(batch_sizes)))\n",
        "axes[1, 1].set_xticklabels([f'{bs}' for bs in batch_sizes])\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbmHpH5YeCvh"
      },
      "outputs": [],
      "source": [
        "# Find best performing configuration\n",
        "best_config = max(results.keys(), key=lambda x: results[x]['best_val_acc'])\n",
        "best_model = results[best_config]['model']\n",
        "best_config_details = results[best_config]['config']\n",
        "\n",
        "print(f\"\\nBest performing configuration: {best_config_details['description']}\")\n",
        "print(f\"Best validation accuracy: {results[best_config]['best_val_acc']:.2f}%\")\n",
        "print(f\"Batch size: {best_config_details['batch_size']}\")\n",
        "print(f\"Learning rate: {best_config_details['lr']}\")\n",
        "\n",
        "# Create validation loader with best batch size for final evaluation\n",
        "final_val_loader = DataLoader(val_dataset, batch_size=best_config_details['batch_size'], \n",
        "                             shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the best model and evaluate\n",
        "best_model.load_state_dict(torch.load(f'best_{best_config.lower()}_model.pth'))\n",
        "_, _, _, final_predictions, final_labels = validate_epoch(best_model, final_val_loader, nn.CrossEntropyLoss(), device)\n",
        "\n",
        "# Confusion matrix for best model\n",
        "cm = confusion_matrix(final_labels, final_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_labels,\n",
        "            yticklabels=emotion_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix - Best Model ({best_config_details[\"description\"]})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMlv8C-XeCvh"
      },
      "outputs": [],
      "source": [
        "# Classification report for best model\n",
        "print(f\"\\nClassification Report - Best Model ({best_config_details['description']}):\")\n",
        "print(\"=\" * 80)\n",
        "report = classification_report(final_labels, final_predictions,\n",
        "                             target_names=emotion_labels,\n",
        "                             output_dict=True)\n",
        "print(classification_report(final_labels, final_predictions, target_names=emotion_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QEvKNzieCvh"
      },
      "outputs": [],
      "source": [
        "# Analyze batch size effects in detail\n",
        "print(\"\\nDetailed Batch Size Analysis:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    \n",
        "    # Calculate various metrics\n",
        "    convergence_speed = next((i for i, acc in enumerate(data['val_accs']) \n",
        "                            if acc >= 0.9 * data['best_val_acc']), num_epochs)\n",
        "    \n",
        "    final_overfitting = data['final_train_acc'] - data['final_val_acc']\n",
        "    max_overfitting = max([data['train_accs'][i] - data['val_accs'][i] \n",
        "                          for i in range(len(data['train_accs']))])\n",
        "    \n",
        "    gradient_stability = np.std(data['gradient_norms'])\n",
        "    loss_smoothness = np.std(np.diff(data['train_losses']))\n",
        "    \n",
        "    print(f\"\\n{config['description']} (Batch Size: {config['batch_size']}):\")\n",
        "    print(f\"  Performance Metrics:\")\n",
        "    print(f\"    - Best Validation Accuracy: {data['best_val_acc']:.2f}%\")\n",
        "    print(f\"    - Final Overfitting Gap: {final_overfitting:.2f}%\")\n",
        "    print(f\"    - Maximum Overfitting Gap: {max_overfitting:.2f}%\")\n",
        "    print(f\"  \\n  Training Dynamics:\")\n",
        "    print(f\"    - Convergence Speed (90% best): Epoch {convergence_speed + 1}\")\n",
        "    print(f\"    - Gradient Stability (std): {gradient_stability:.4f}\")\n",
        "    print(f\"    - Loss Smoothness (std of diff): {loss_smoothness:.4f}\")\n",
        "    print(f\"  \\n  Efficiency Metrics:\")\n",
        "    print(f\"    - Average Training Time: {data['avg_train_time']:.1f}s per epoch\")\n",
        "    print(f\"    - Throughput: {len(train_dataset)/data['avg_train_time']:.1f} samples/s\")\n",
        "    print(f\"    - Updates per Epoch: {len(train_dataset) // config['batch_size']}\")\n",
        "    print(f\"    - Effective Learning Rate: {config['lr'] * config['batch_size']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR3_7g-QeCvi"
      },
      "outputs": [],
      "source": [
        "# Create learning curve comparison showing noise vs smoothness\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Training loss curves (smoothed and raw)\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    epochs = range(1, len(data['train_losses']) + 1)\n",
        "    \n",
        "    # Raw training loss\n",
        "    axes[0, 0].plot(epochs, data['train_losses'], \n",
        "                   label=f\"{config['description']}\", \n",
        "                   color=config['color'], alpha=0.7, linewidth=1.5)\n",
        "\n",
        "# Validation accuracy with confidence intervals\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    epochs = range(1, len(data['val_accs']) + 1)\n",
        "    \n",
        "    axes[0, 1].plot(epochs, data['val_accs'], \n",
        "                   label=f\"{config['description']} (Best: {data['best_val_acc']:.1f}%)\", \n",
        "                   color=config['color'], linewidth=2, \n",
        "                   marker=config['marker'], markersize=3)\n",
        "\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Training Loss')\n",
        "axes[0, 0].set_title('Training Loss Progression (Raw)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Validation Accuracy (%)')\n",
        "axes[0, 1].set_title('Validation Accuracy Progression')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient norm evolution\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    epochs = range(1, len(data['gradient_norms']) + 1)\n",
        "    \n",
        "    axes[1, 0].plot(epochs, data['gradient_norms'], \n",
        "                   label=config['description'], \n",
        "                   color=config['color'], linewidth=1.5, alpha=0.8)\n",
        "\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Average Gradient Norm')\n",
        "axes[1, 0].set_title('Gradient Norm Evolution')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_yscale('log')\n",
        "\n",
        "# Training efficiency: Accuracy improvement per minute\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    \n",
        "    # Calculate cumulative time and accuracy improvement\n",
        "    cumulative_time = np.cumsum(data['train_times']) / 60  # Convert to minutes\n",
        "    accuracy_improvement = np.array(data['val_accs']) - data['val_accs'][0]\n",
        "    \n",
        "    axes[1, 1].plot(cumulative_time, accuracy_improvement, \n",
        "                   label=config['description'], \n",
        "                   color=config['color'], linewidth=2,\n",
        "                   marker=config['marker'], markersize=4)\n",
        "\n",
        "axes[1, 1].set_xlabel('Training Time (minutes)')\n",
        "axes[1, 1].set_ylabel('Validation Accuracy Improvement (%)')\n",
        "axes[1, 1].set_title('Training Efficiency: Accuracy vs Time')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbmHpH5YeCvi"
      },
      "outputs": [],
      "source": [
        "# Save all models and create final summary\n",
        "for config_name in results.keys():\n",
        "    torch.save(results[config_name]['model'].state_dict(), \n",
        "               f'final_{config_name.lower()}_model.pth')\n",
        "\n",
        "# Initialize final W&B run for comparison\n",
        "wandb.init(\n",
        "    project=\"fer-challenge\",\n",
        "    name=\"exp10-batch-size-comparison-summary\",\n",
        "    config={\n",
        "        \"experiment\": \"Batch Size Impact\",\n",
        "        \"batch_sizes\": [config['batch_size'] for config in batch_size_configs.values()],\n",
        "        \"dataset\": \"FER2013\",\n",
        "        \"epochs\": num_epochs,\n",
        "        \"architecture\": \"CNN with BatchNorm and Dropout\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Log comparison metrics\n",
        "comparison_metrics = {}\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    comparison_metrics.update({\n",
        "        f\"batch_{config['batch_size']}_best_val_acc\": data['best_val_acc'],\n",
        "        f\"batch_{config['batch_size']}_final_train_acc\": data['final_train_acc'],\n",
        "        f\"batch_{config['batch_size']}_final_val_acc\": data['final_val_acc'],\n",
        "        f\"batch_{config['batch_size']}_avg_train_time\": data['avg_train_time'],\n",
        "        f\"batch_{config['batch_size']}_throughput\": len(train_dataset) / data['avg_train_time'],\n",
        "        f\"batch_{config,['batch_size']}_avg_gradient_norm\": data['avg_gradient_norm']\n",
        "    })\n",
        "\n",
        "wandb.log(comparison_metrics)\n",
        "\n",
        "# Save models to W&B\n",
        "for config_name in results.keys():\n",
        "    wandb.save(f'final_{config_name.lower()}_model.pth')\n",
        "    wandb.save(f'best_{config_name.lower()}_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN9Ok8bVeCvi"
      },
      "outputs": [],
      "source": [
        "# Final comprehensive analysis and recommendations\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"EXPERIMENT SUMMARY: BATCH SIZE IMPACT ANALYSIS\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - 3 Convolutional Blocks with BatchNorm\")\n",
        "print(f\"  - Conv channels: [64, 128, 256]\")\n",
        "print(f\"  - FC layers: [512, 256, 7]\")\n",
        "print(f\"  - Total Parameters: {total_params:,}\")\n",
        "print(f\"  - Regularization: BatchNorm + Dropout\")\n",
        "\n",
        "print(f\"\\nBatch Size Configurations Tested:\")\n",
        "for name, config in batch_size_configs.items():\n",
        "    print(f\"  - {config['description']}: {config['batch_size']} (LR: {config['lr']})\")\n",
        "\n",
        "print(f\"\\nPerformance Rankings (by Best Validation Accuracy):\")\n",
        "sorted_configs = sorted(results.items(), key=lambda x: x[1]['best_val_acc'], reverse=True)\n",
        "for rank, (config_name, data) in enumerate(sorted_configs, 1):\n",
        "    config = data['config']\n",
        "    convergence_epoch = next((i for i, acc in enumerate(data['val_accs']) \n",
        "                            if acc >= 0.95 * data['best_val_acc']), num_epochs) + 1\n",
        "    print(f\"  {rank}. {config['description']}: {data['best_val_acc']:.2f}% (Converged: Epoch {convergence_epoch})\")\n",
        "\n",
        "print(f\"\\nEfficiency Rankings (by Throughput):\")\n",
        "sorted_by_throughput = sorted(results.items(), \n",
        "                            key=lambda x: len(train_dataset) / x[1]['avg_train_time'], \n",
        "                            reverse=True)\n",
        "for rank, (config_name, data) in enumerate(sorted_by_throughput, 1):\n",
        "    config = data['config']\n",
        "    throughput = len(train_dataset) / data['avg_train_time']\n",
        "    print(f\"  {rank}. {config['description']}: {throughput:.1f} samples/s\")\n",
        "\n",
        "print(f\"\\nKey Observations:\")\n",
        "best_perf_config = sorted_configs[0][0]\n",
        "best_eff_config = sorted_by_throughput[0][0]\n",
        "worst_perf_config = sorted_configs[-1][0]\n",
        "\n",
        "performance_gap = (results[best_perf_config]['best_val_acc'] - \n",
        "                  results[worst_perf_config]['best_val_acc'])\n",
        "\n",
        "print(f\"  - Best performing batch size: {results[best_perf_config]['config']['batch_size']}\")\n",
        "print(f\"  - Most efficient batch size: {results[best_eff_config]['config']['batch_size']}\")\n",
        "print(f\"  - Performance gap between best and worst: {performance_gap:.2f}%\")\n",
        "\n",
        "# Calculate gradient noise correlation\n",
        "batch_sizes_list = [results[config]['config']['batch_size'] for config in results.keys()]\n",
        "gradient_stds = [np.std(results[config]['gradient_norms']) for config in results.keys()]\n",
        "noise_correlation = np.corrcoef(np.log(batch_sizes_list), gradient_stds)[0, 1]\n",
        "print(f\"  - Gradient noise correlation with batch size: {noise_correlation:.3f}\")\n",
        "\n",
        "# Find most stable training\n",
        "stability_scores = {config: np.std(data['val_accs'][-5:]) for config, data in results.items()}\n",
        "most_stable = min(stability_scores.keys(), key=lambda x: stability_scores[x])\n",
        "print(f\"  - Most stable training: {results[most_stable]['config']['description']}\")\n",
        "\n",
        "print(f\"\\nBatch Size Effects Analysis:\")\n",
        "print(f\"  Small Batches (16-32):\")\n",
        "small_configs = [k for k, v in results.items() if v['config']['batch_size'] <= 32]\n",
        "if small_configs:\n",
        "    small_data = results[small_configs[0]]\n",
        "    print(f\"    - Higher gradient noise: {np.std(small_data['gradient_norms']):.4f}\")\n",
        "    print(f\"    - More frequent updates: {len(train_dataset) // small_data['config']['batch_size']} per epoch\")\n",
        "    print(f\"    - Better generalization potential\")\n",
        "\n",
        "print(f\"  Large Batches (128+):\")\n",
        "large_configs = [k for k, v in results.items() if v['config']['batch_size'] >= 128]\n",
        "if large_configs:\n",
        "    large_data = results[large_configs[0]]\n",
        "    print(f\"    - Lower gradient noise: {np.std(large_data['gradient_norms']):.4f}\")\n",
        "    print(f\"    - Fewer updates: {len(train_dataset) // large_data['config']['batch_size']} per epoch\")\n",
        "    print(f\"    - Higher computational efficiency\")\n",
        "    print(f\"    - Potential for faster convergence with proper LR scaling\")\n",
        "\n",
        "print(f\"\\nMemory and Computational Trade-offs:\")\n",
        "min_time = min(data['avg_train_time'] for data in results.values())\n",
        "max_time = max(data['avg_train_time'] for data in results.values())\n",
        "print(f\"  - Training time range: {min_time:.1f}s - {max_time:.1f}s per epoch\")\n",
        "print(f\"  - Speedup from optimal batch size: {max_time/min_time:.2f}x\")\n",
        "\n",
        "# GPU utilization estimate\n",
        "for config_name, data in results.items():\n",
        "    config = data['config']\n",
        "    theoretical_throughput = len(train_dataset) / (len(train_dataset) // config['batch_size']) * config['batch_size']\n",
        "    actual_throughput = len(train_dataset) / data['avg_train_time']\n",
        "    efficiency = actual_throughput / theoretical_throughput * 100\n",
        "    \n",
        "print(f\"\\nRecommendations:\")\n",
        "if results[best_perf_config]['config']['batch_size'] <= 64:\n",
        "    print(f\"  - For best accuracy: Use moderate batch sizes (16-64) for better generalization\")\n",
        "else:\n",
        "    print(f\"  - For best accuracy: Large batches work well with proper LR scaling\")\n",
        "\n",
        "if results[best_eff_config]['config']['batch_size'] >= 128:\n",
        "    print(f\"  - For efficiency: Use larger batch sizes (128+) for faster training\")\n",
        "else:\n",
        "    print(f\"  - For efficiency: Medium batch sizes provide good speed-accuracy balance\")\n",
        "\n",
        "print(f\"  - Consider gradient accumulation for very large effective batch sizes\")\n",
        "print(f\"  - Adjust learning rate proportionally with batch size (linear scaling rule)\")\n",
        "print(f\"  - Monitor gradient norms to detect optimization issues\")\n",
        "print(f\"  - Use learning rate warmup for large batch training\")\n",
        "\n",
        "print(f\"\\nOptimal Configuration for this Task:\")\n",
        "optimal_config = results[best_perf_config]['config']\n",
        "print(f\"  - Batch Size: {optimal_config['batch_size']}\")\n",
        "print(f\"  - Learning Rate: {optimal_config['lr']}\")\n",
        "print(f\"  - Expected Accuracy: {results[best_perf_config]['best_val_acc']:.2f}%\")\n",
        "print(f\"  - Training Time: {results[best_perf_config]['avg_train_time']:.1f}s per epoch\")\n",
        "print(f\"  - Throughput: {len(train_dataset)/results[best_perf_config]['avg_train_time']:.1f} samples/s\")\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
